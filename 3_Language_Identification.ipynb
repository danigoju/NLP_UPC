{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3958863c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:10.599499Z",
     "iopub.status.busy": "2022-04-18T09:34:10.598711Z",
     "iopub.status.idle": "2022-04-18T09:34:13.400846Z",
     "shell.execute_reply": "2022-04-18T09:34:13.401726Z",
     "shell.execute_reply.started": "2022-04-17T21:09:05.314946Z"
    },
    "papermill": {
     "duration": 2.839626,
     "end_time": "2022-04-18T09:34:13.402069",
     "exception": false,
     "start_time": "2022-04-18T09:34:10.562443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scripts.txt', 'x_train.txt', 'x_test.txt', 'y_train.txt', 'labels.csv']\n",
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "import time, math\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "INPUTDIR = '../input/wili5'\n",
    "print(os.listdir(f'{INPUTDIR}'))\n",
    "\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "x_train_full = open(f'{INPUTDIR}/x_train.txt').read().splitlines()\n",
    "y_train_full = open(f'{INPUTDIR}/y_train.txt').read().splitlines()\n",
    "print('Example:')\n",
    "print('LANG =', y_train_full[0])\n",
    "print('TEXT =', x_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3259147",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:13.467779Z",
     "iopub.status.busy": "2022-04-18T09:34:13.467042Z",
     "iopub.status.idle": "2022-04-18T09:34:13.470050Z",
     "shell.execute_reply": "2022-04-18T09:34:13.470455Z",
     "shell.execute_reply.started": "2022-04-17T21:09:08.849763Z"
    },
    "papermill": {
     "duration": 0.04095,
     "end_time": "2022-04-18T09:34:13.470611",
     "exception": false,
     "start_time": "2022-04-18T09:34:13.429661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2b74a",
   "metadata": {
    "papermill": {
     "duration": 0.023899,
     "end_time": "2022-04-18T09:34:13.519536",
     "exception": false,
     "start_time": "2022-04-18T09:34:13.495637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Dictionary** class is used to map tokens (characters, words, subwords) into consecutive integer indexes.  \n",
    "The index **0** is reserved for padding sequences up to a fixed lenght, and the index **1** for any 'unknown' character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342ef3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:14.929031Z",
     "iopub.status.busy": "2022-04-18T09:34:13.688824Z",
     "iopub.status.idle": "2022-04-18T09:34:14.942654Z",
     "shell.execute_reply": "2022-04-18T09:34:14.943066Z",
     "shell.execute_reply.started": "2022-04-17T21:09:08.864326Z"
    },
    "papermill": {
     "duration": 1.39951,
     "end_time": "2022-04-18T09:34:14.943246",
     "exception": false,
     "start_time": "2022-04-18T09:34:13.543736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10808 UTF characters\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "char_vocab = Dictionary()\n",
    "pad_token = '<pad>' # reserve index 0 for padding\n",
    "unk_token = '<unk>' # reserve index 1 for unknown token\n",
    "pad_index = char_vocab.add_token(pad_token)\n",
    "unk_index = char_vocab.add_token(unk_token)\n",
    "\n",
    "# join all the training sentences in a single string\n",
    "# and obtain the list of different characters with set\n",
    "chars = set(''.join(x_train_full))\n",
    "for char in sorted(chars):\n",
    "    char_vocab.add_token(char)\n",
    "print(\"Vocabulary:\", len(char_vocab), \"UTF characters\")\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train_full)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53e3685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:15.042432Z",
     "iopub.status.busy": "2022-04-18T09:34:15.032355Z",
     "iopub.status.idle": "2022-04-18T09:34:24.508684Z",
     "shell.execute_reply": "2022-04-18T09:34:24.509063Z",
     "shell.execute_reply.started": "2022-04-17T21:09:10.419481Z"
    },
    "papermill": {
     "duration": 9.540483,
     "end_time": "2022-04-18T09:34:24.509220",
     "exception": false,
     "start_time": "2022-04-18T09:34:14.968737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> 67\n",
      "cat -> 28\n",
      "est Klement Go\n",
      "52 [45 78 71 79 71 80 86  2 41 81]\n"
     ]
    }
   ],
   "source": [
    "#From token or label to index\n",
    "print('a ->', char_vocab.token2idx['a'])\n",
    "print('cat ->', lang_vocab.token2idx['cat'])\n",
    "print(y_train_full[0], x_train_full[0][:10])\n",
    "x_train_idx = [np.array([char_vocab.token2idx[c] for c in line]) for line in x_train_full]\n",
    "y_train_idx = np.array([lang_vocab.token2idx[lang] for lang in y_train_full])\n",
    "print(y_train_idx[0], x_train_idx[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58be915",
   "metadata": {
    "papermill": {
     "duration": 0.024844,
     "end_time": "2022-04-18T09:34:24.559784",
     "exception": false,
     "start_time": "2022-04-18T09:34:24.534940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Radomly select 15% of the database for validation  \n",
    "Create lists of (input, target) tuples for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac4ed35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:24.615912Z",
     "iopub.status.busy": "2022-04-18T09:34:24.612087Z",
     "iopub.status.idle": "2022-04-18T09:34:25.454492Z",
     "shell.execute_reply": "2022-04-18T09:34:25.455474Z",
     "shell.execute_reply.started": "2022-04-17T21:09:19.779908Z"
    },
    "papermill": {
     "duration": 0.870914,
     "end_time": "2022-04-18T09:34:25.455689",
     "exception": false,
     "start_time": "2022-04-18T09:34:24.584775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99875 training samples\n",
      "17625 validation samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_idx, y_train_idx, test_size=0.15, random_state=seed)\n",
    "train_data = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "val_data = [(x, y) for x, y in zip(x_val, y_val)]\n",
    "print(len(train_data), \"training samples\")\n",
    "print(len(val_data), \"validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f5f8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:25.520647Z",
     "iopub.status.busy": "2022-04-18T09:34:25.519713Z",
     "iopub.status.idle": "2022-04-18T09:34:25.521691Z",
     "shell.execute_reply": "2022-04-18T09:34:25.522147Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.579141Z"
    },
    "papermill": {
     "duration": 0.038287,
     "end_time": "2022-04-18T09:34:25.522275",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.483988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, token_size):\n",
    "    \"\"\"Yield elements from data in chunks with a maximum of batch_size sequences and token_size tokens.\"\"\"\n",
    "    minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "    for ex in data:\n",
    "        seq_len = len(ex[0])\n",
    "        if seq_len > token_size:\n",
    "            ex = (ex[0][:token_size], ex[1])\n",
    "            seq_len = token_size\n",
    "        minibatch.append(ex)\n",
    "        sequences_so_far += 1\n",
    "        tokens_so_far += seq_len\n",
    "        if sequences_so_far == batch_size or tokens_so_far == token_size:\n",
    "            yield minibatch\n",
    "            minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "        elif sequences_so_far > batch_size or tokens_so_far > token_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, sequences_so_far, tokens_so_far = minibatch[-1:], 1, len(minibatch[-1][0])\n",
    "    if minibatch:\n",
    "        yield minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed47f9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:25.581859Z",
     "iopub.status.busy": "2022-04-18T09:34:25.581094Z",
     "iopub.status.idle": "2022-04-18T09:34:25.583045Z",
     "shell.execute_reply": "2022-04-18T09:34:25.583483Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.589065Z"
    },
    "papermill": {
     "duration": 0.03456,
     "end_time": "2022-04-18T09:34:25.583614",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.549054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pool_generator(data, batch_size, token_size, shuffle=False):\n",
    "    \"\"\"Sort within buckets, then batch, then shuffle batches.\n",
    "    Partitions data into chunks of size 100*token_size, sorts examples within\n",
    "    each chunk, then batch these examples and shuffle the batches.\n",
    "    \"\"\"\n",
    "    for p in batch_generator(data, batch_size * 100, token_size * 100):\n",
    "        p_batch = batch_generator(sorted(p, key=lambda t: len(t[0]), reverse=True), batch_size, token_size)\n",
    "        p_list = list(p_batch)\n",
    "        if shuffle:\n",
    "            for b in random.sample(p_list, len(p_list)):\n",
    "                yield b\n",
    "        else:\n",
    "            for b in p_list:\n",
    "                yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8dde8",
   "metadata": {
    "papermill": {
     "duration": 0.025699,
     "end_time": "2022-04-18T09:34:25.635365",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.609666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**DNN Model**  \n",
    "Includes Python comments with the dimension of the input  matrix:  \n",
    "T = Max number of tokens in a sequence  \n",
    "B = Number of sequences (batch size)  \n",
    "E = Embedding dim  \n",
    "H = Hidden size  \n",
    "O = Output size (number of languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784ebcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:25.726323Z",
     "iopub.status.busy": "2022-04-18T09:34:25.721211Z",
     "iopub.status.idle": "2022-04-18T09:34:25.739003Z",
     "shell.execute_reply": "2022-04-18T09:34:25.738597Z"
    },
    "papermill": {
     "duration": 0.078168,
     "end_time": "2022-04-18T09:34:25.739114",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.660946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CharRNNClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=1, bidirectional=False, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        if bidirectional: # We have hidden states of both directions\n",
    "            self.h2o = torch.nn.Linear(hidden_size*2, output_size)\n",
    "        else:\n",
    "            self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, input_lengths):\n",
    "        # T x B\n",
    "        encoded = self.embed(input)\n",
    "        # T x B x E\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n",
    "        # Packed T x B x E\n",
    "        output, _ = self.rnn(packed)\n",
    "        # Packed T x B x H\n",
    "        # Important: you may need to replace '-inf' with the default zero padding for other pooling layers\n",
    "        padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n",
    "        # T x B x H\n",
    "        output, _ = padded.max(dim=0)\n",
    "        # B x H\n",
    "        output = self.h2o(output)\n",
    "        # B x O\n",
    "        return output\n",
    "\n",
    "    \n",
    "class CharRNNClassifier_ConcatPooling(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=1, bidirectional=False, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        if bidirectional: # We have hidden states of both directions\n",
    "            self.h2o = torch.nn.Linear(hidden_size*4, output_size)\n",
    "        else:\n",
    "            self.h2o = torch.nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = torch.nn.Dropout(0.2) # Add dropout after the pooling layer\n",
    "        \n",
    "        \n",
    "    def forward(self, input, input_lengths):\n",
    "        # T x B\n",
    "        encoded = self.embed(input)\n",
    "        print(input.shape)\n",
    "        # T x B x E\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n",
    "        # Packed T x B x E\n",
    "        output, _ = self.rnn(packed)\n",
    "        # Packed T x B x H\n",
    "        # Important: you may need to replace '-inf' with the default zero padding for other pooling layers\n",
    "        max_padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n",
    "        # T x B x H\n",
    "        max_pooled, _ = max_padded.max(dim=0)\n",
    "        avg_padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        avg_pooled = avg_padded.mean(dim=0)\n",
    "        # Concat avg pooling and max pooling\n",
    "        pool_concat = torch.cat((max_pooled,avg_pooled),dim=1)\n",
    "        # Add dropout\n",
    "        output = self.dropout(pool_concat)\n",
    "        # B x H\n",
    "        output = self.h2o(output)\n",
    "        # B x O\n",
    "        return output\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -Inf)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "    \n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.k_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Empirically observed the convergence to be much better with the scaled initialization\n",
    "        torch.nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
    "        torch.nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
    "        torch.nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
    "        torch.nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            torch.nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    # B = Batch size\n",
    "    # W = Number of context words (left + right)\n",
    "    # E = embedding_dim\n",
    "    def forward(self, x):\n",
    "        # x shape is (B, W, E)\n",
    "        q = self.q_proj(x)\n",
    "        # q shape is (B, W, E)\n",
    "        k = self.k_proj(x)\n",
    "        # k shape is (B, W, E)\n",
    "        v = self.v_proj(x)\n",
    "        # k shape is (B, W, E)\n",
    "        y, _ = attention(q, k, v)\n",
    "        # y shape is (B, W, E)\n",
    "        y = self.out_proj(y)\n",
    "        # y shape is (B, W, E)\n",
    "        return y\n",
    "\n",
    "class CosinePositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Embedding dimensionality\n",
    "            dropout (float): Dropout probability on the forward pass\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Don the transpose because the class originally expected shape [seq_len, batch_size, embedding_dim]\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return torch.transpose(self.dropout(x), 0 ,1) # Return original size\n",
    "    \n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(d_model)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = torch.nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear2 = torch.nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model,n_heads, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = int(d_model/n_heads)\n",
    "        self.att_heads = torch.nn.ModuleList([TransformerLayer(self.d_head) for i in range(n_heads)])\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = None\n",
    "        for i,emb_section in enumerate(torch.split(src,self.d_head,dim = 2)):\n",
    "            #print(emb_section.shape,src.shape)\n",
    "            out_section = self.att_heads[i](emb_section)\n",
    "            if out is None:\n",
    "                out = out_section\n",
    "            else:\n",
    "                out = torch.cat((out,out_section),2)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class EncoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.att1 = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim, ffn_dim),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multihead Attention block\n",
    "        selfattn_out = self.att1(x)\n",
    "\n",
    "        # Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Char_Attention_Classifier(torch.nn.Module):\n",
    "    \"\"\" Reusing the best model from last lab \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_size, output_size, num_layers=6, n_heads=2, ffn_dim= 1024):\n",
    "        super().__init__()\n",
    "        # Use Cosine Positional encoding as input sequence can have different lengths.\n",
    "        self.position_embedding = CosinePositionalEncoding(embedding_size)\n",
    "        self.emb = torch.nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
    "        self.lin = torch.nn.Linear(embedding_size, output_size, bias=False)        \n",
    "        #self.position_embedding = torch.nn.Parameter(torch.Tensor(input_size, embedding_size))\n",
    "        #torch.nn.init.xavier_uniform_(self.position_embedding)\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            EncoderLayer(embedding_size, ffn_dim, n_heads, dropout = 0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    # B = Batch size\n",
    "    # W = Number of context words (left + right)\n",
    "    # E = embedding_dim\n",
    "    # V = num_embeddings (number of words)\n",
    "    def forward(self, input, input_lengths):\n",
    "        x = torch.transpose(input, 0, 1)\n",
    "        # input shape is (B,W)\n",
    "        x = self.emb(x)\n",
    "        # x shape is (B, W, E)\n",
    "        x = self.position_embedding(x)\n",
    "        # x shape is (B, W, E)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # x shape is (B, W, E)\n",
    "        x = torch.sum(x,dim=1)\n",
    "        # x shape is (B, E)\n",
    "        x = self.lin(x)\n",
    "        # x shape is (B, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e83e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:25.856360Z",
     "iopub.status.busy": "2022-04-18T09:34:25.855637Z",
     "iopub.status.idle": "2022-04-18T09:34:25.858662Z",
     "shell.execute_reply": "2022-04-18T09:34:25.858179Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.651083Z"
    },
    "papermill": {
     "duration": 0.093954,
     "end_time": "2022-04-18T09:34:25.858795",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.764841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb3135",
   "metadata": {
    "papermill": {
     "duration": 0.025358,
     "end_time": "2022-04-18T09:34:25.910099",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.884741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **nn.CrossEntropyLoss()** criterion combines **nn.LogSoftmax()** and **nn.NLLLoss()** in one single class.  \n",
    "It is useful when training a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a3cefd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:25.966060Z",
     "iopub.status.busy": "2022-04-18T09:34:25.965256Z",
     "iopub.status.idle": "2022-04-18T09:34:25.967729Z",
     "shell.execute_reply": "2022-04-18T09:34:25.967289Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.715201Z"
    },
    "papermill": {
     "duration": 0.031898,
     "end_time": "2022-04-18T09:34:25.967851",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.935953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8bdfe29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:26.030320Z",
     "iopub.status.busy": "2022-04-18T09:34:26.029471Z",
     "iopub.status.idle": "2022-04-18T09:34:26.031272Z",
     "shell.execute_reply": "2022-04-18T09:34:26.031726Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.726761Z"
    },
    "papermill": {
     "duration": 0.037622,
     "end_time": "2022-04-18T09:34:26.031870",
     "exception": false,
     "start_time": "2022-04-18T09:34:25.994248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, batch_size, token_size, max_norm=1, log=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    ntokens = 0\n",
    "    niterations = 0\n",
    "    for batch in pool_generator(data, batch_size, token_size, shuffle=True):\n",
    "        # Get input and target sequences from batch\n",
    "        X = [torch.from_numpy(d[0]) for d in batch]\n",
    "        X_lengths = [x.numel() for x in X]\n",
    "        ntokens += sum(X_lengths)\n",
    "        X_lengths = torch.tensor(X_lengths, dtype=torch.long)\n",
    "        y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "        # Pad the input sequences to create a matrix\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(X, X_lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)      # Gradient clipping https://www.kaggle.com/c/wili4/discussion/231378\n",
    "        optimizer.step()\n",
    "        # Training statistics\n",
    "        total_loss += loss.item()\n",
    "        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n",
    "        nsentences += y.numel()\n",
    "        niterations += 1\n",
    "    \n",
    "    total_loss = total_loss / nsentences\n",
    "    accuracy = 100 * ncorrect / nsentences\n",
    "    if log:\n",
    "        print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c603148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:26.091291Z",
     "iopub.status.busy": "2022-04-18T09:34:26.090544Z",
     "iopub.status.idle": "2022-04-18T09:34:26.092575Z",
     "shell.execute_reply": "2022-04-18T09:34:26.092954Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.739171Z"
    },
    "papermill": {
     "duration": 0.034909,
     "end_time": "2022-04-18T09:34:26.093080",
     "exception": false,
     "start_time": "2022-04-18T09:34:26.058171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    # calculate accuracy on validation set\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input and target sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long)\n",
    "            y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            ncorrect += (torch.max(answer, 1)[1] == y).sum().item()\n",
    "            nsentences += y.numel()\n",
    "        dev_acc = 100 * ncorrect / nsentences\n",
    "    return dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36de6773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:26.148855Z",
     "iopub.status.busy": "2022-04-18T09:34:26.148058Z",
     "iopub.status.idle": "2022-04-18T09:34:26.150037Z",
     "shell.execute_reply": "2022-04-18T09:34:26.150444Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.75242Z"
    },
    "papermill": {
     "duration": 0.032009,
     "end_time": "2022-04-18T09:34:26.150579",
     "exception": false,
     "start_time": "2022-04-18T09:34:26.118570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hidden_size = 256\n",
    "#embedding_size = 64\n",
    "\n",
    "ntokens = len(char_vocab)\n",
    "nlabels = len(lang_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e38e0f",
   "metadata": {
    "papermill": {
     "duration": 0.025675,
     "end_time": "2022-04-18T09:34:26.201879",
     "exception": false,
     "start_time": "2022-04-18T09:34:26.176204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950845eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:26.260760Z",
     "iopub.status.busy": "2022-04-18T09:34:26.259952Z",
     "iopub.status.idle": "2022-04-18T09:34:26.262395Z",
     "shell.execute_reply": "2022-04-18T09:34:26.261927Z",
     "shell.execute_reply.started": "2022-04-17T21:09:20.762588Z"
    },
    "papermill": {
     "duration": 0.034625,
     "end_time": "2022-04-18T09:34:26.262512",
     "exception": false,
     "start_time": "2022-04-18T09:34:26.227887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(num_model):\n",
    "    if num_model == 1:\n",
    "        model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index, num_layers = 3).to(device)\n",
    "    if num_model == 2:\n",
    "        model = CharRNNClassifier_ConcatPooling(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index, num_layers = 3).to(device)\n",
    "    if num_model == 3:\n",
    "        model = Char_Attention_Classifier(ntokens, embedding_size, nlabels, num_layers = 3, n_heads=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615231bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T09:34:26.321540Z",
     "iopub.status.busy": "2022-04-18T09:34:26.320881Z",
     "iopub.status.idle": "2022-04-18T11:07:28.916974Z",
     "shell.execute_reply": "2022-04-18T11:07:28.917396Z",
     "shell.execute_reply.started": "2022-04-17T21:12:06.848406Z"
    },
    "papermill": {
     "duration": 5582.629167,
     "end_time": "2022-04-18T11:07:28.917557",
     "exception": false,
     "start_time": "2022-04-18T09:34:26.288390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross-validation model for 25 epochs\n",
      "Train: wpb=9646, bsz=26, num_updates=3793\n",
      "| epoch 001 | train accuracy=36.2% (220s)\n",
      "| epoch 001 | valid accuracy=73.0%\n",
      "| epoch 002 | train accuracy=77.3% (446s)\n",
      "| epoch 002 | valid accuracy=81.2%\n",
      "| epoch 003 | train accuracy=82.5% (670s)\n",
      "| epoch 003 | valid accuracy=82.6%\n",
      "| epoch 004 | train accuracy=84.3% (893s)\n",
      "| epoch 004 | valid accuracy=83.9%\n",
      "| epoch 005 | train accuracy=85.5% (1115s)\n",
      "| epoch 005 | valid accuracy=84.3%\n",
      "| epoch 006 | train accuracy=86.1% (1338s)\n",
      "| epoch 006 | valid accuracy=86.1%\n",
      "| epoch 007 | train accuracy=86.7% (1562s)\n",
      "| epoch 007 | valid accuracy=86.0%\n",
      "| epoch 008 | train accuracy=87.2% (1785s)\n",
      "| epoch 008 | valid accuracy=86.2%\n",
      "| epoch 009 | train accuracy=87.5% (2007s)\n",
      "| epoch 009 | valid accuracy=86.2%\n",
      "| epoch 010 | train accuracy=87.8% (2230s)\n",
      "| epoch 010 | valid accuracy=87.1%\n",
      "| epoch 011 | train accuracy=88.0% (2453s)\n",
      "| epoch 011 | valid accuracy=86.3%\n",
      "| epoch 012 | train accuracy=88.1% (2675s)\n",
      "| epoch 012 | valid accuracy=87.3%\n",
      "| epoch 013 | train accuracy=88.2% (2899s)\n",
      "| epoch 013 | valid accuracy=86.0%\n",
      "| epoch 014 | train accuracy=88.6% (3122s)\n",
      "| epoch 014 | valid accuracy=87.3%\n",
      "| epoch 015 | train accuracy=88.5% (3345s)\n",
      "| epoch 015 | valid accuracy=87.0%\n",
      "| epoch 016 | train accuracy=88.7% (3567s)\n",
      "| epoch 016 | valid accuracy=86.9%\n",
      "| epoch 017 | train accuracy=88.9% (3790s)\n",
      "| epoch 017 | valid accuracy=87.5%\n",
      "| epoch 018 | train accuracy=89.0% (4013s)\n",
      "| epoch 018 | valid accuracy=87.7%\n",
      "| epoch 019 | train accuracy=89.1% (4235s)\n",
      "| epoch 019 | valid accuracy=87.7%\n",
      "| epoch 020 | train accuracy=89.1% (4458s)\n",
      "| epoch 020 | valid accuracy=87.5%\n",
      "| epoch 021 | train accuracy=89.2% (4680s)\n",
      "| epoch 021 | valid accuracy=87.6%\n",
      "| epoch 022 | train accuracy=89.3% (4903s)\n",
      "| epoch 022 | valid accuracy=87.9%\n",
      "| epoch 023 | train accuracy=89.4% (5126s)\n",
      "| epoch 023 | valid accuracy=87.4%\n",
      "| epoch 024 | train accuracy=89.4% (5349s)\n",
      "| epoch 024 | valid accuracy=88.1%\n",
      "| epoch 025 | train accuracy=89.4% (5572s)\n",
      "| epoch 025 | valid accuracy=87.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "token_size = 10000\n",
    "#hidden_size = 512//2\n",
    "embedding_size = 64\n",
    "#bidirectional = True\n",
    "#hidden_size = 512\n",
    "\n",
    "epochs = 25\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "model, optimizer = get_model(3)\n",
    "print(f'Training cross-validation model for {epochs} epochs')\n",
    "t0 = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    acc = train(model, optimizer, train_data, batch_size, token_size, log=epoch==1)\n",
    "    train_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}% ({time.time() - t0:.0f}s)')\n",
    "    acc = validate(model, val_data, batch_size, token_size)\n",
    "    valid_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ed9b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T11:07:29.009149Z",
     "iopub.status.busy": "2022-04-18T11:07:29.008214Z",
     "iopub.status.idle": "2022-04-18T11:07:29.025826Z",
     "shell.execute_reply": "2022-04-18T11:07:29.026575Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.754338Z"
    },
    "papermill": {
     "duration": 0.068225,
     "end_time": "2022-04-18T11:07:29.026702",
     "exception": false,
     "start_time": "2022-04-18T11:07:28.958477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char_Attention_Classifier(\n",
      "  (position_embedding): CosinePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (emb): Embedding(10808, 64, padding_idx=0)\n",
      "  (lin): Linear(in_features=64, out_features=235, bias=False)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0): EncoderLayer(\n",
      "      (att1): MultiHeadAttention(\n",
      "        (att_heads): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.1, inplace=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (att1): MultiHeadAttention(\n",
      "        (att_heads): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.1, inplace=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (att1): MultiHeadAttention(\n",
      "        (att_heads): ModuleList(\n",
      "          (0): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerLayer(\n",
      "            (self_attn): SelfAttention(\n",
      "              (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=32, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.1, inplace=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "emb.weight           691712 [10808, 64]\n",
      "lin.weight           15040 [235, 64]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.0.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.0.att1.att_heads.0.linear1.bias 512 [512]\n",
      "encoder_layers.0.att1.att_heads.0.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.0.att1.att_heads.0.linear2.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.norm1.weight 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.norm1.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.norm2.weight 32 [32]\n",
      "encoder_layers.0.att1.att_heads.0.norm2.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.0.att1.att_heads.1.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.0.att1.att_heads.1.linear1.bias 512 [512]\n",
      "encoder_layers.0.att1.att_heads.1.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.0.att1.att_heads.1.linear2.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.norm1.weight 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.norm1.bias 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.norm2.weight 32 [32]\n",
      "encoder_layers.0.att1.att_heads.1.norm2.bias 32 [32]\n",
      "encoder_layers.0.ffn.0.weight 65536 [1024, 64]\n",
      "encoder_layers.0.ffn.0.bias 1024 [1024]\n",
      "encoder_layers.0.ffn.3.weight 65536 [64, 1024]\n",
      "encoder_layers.0.ffn.3.bias 64 [64]\n",
      "encoder_layers.0.norm1.weight 64 [64]\n",
      "encoder_layers.0.norm1.bias 64 [64]\n",
      "encoder_layers.0.norm2.weight 64 [64]\n",
      "encoder_layers.0.norm2.bias 64 [64]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.0.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.1.att1.att_heads.0.linear1.bias 512 [512]\n",
      "encoder_layers.1.att1.att_heads.0.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.1.att1.att_heads.0.linear2.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.norm1.weight 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.norm1.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.norm2.weight 32 [32]\n",
      "encoder_layers.1.att1.att_heads.0.norm2.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.1.att1.att_heads.1.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.1.att1.att_heads.1.linear1.bias 512 [512]\n",
      "encoder_layers.1.att1.att_heads.1.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.1.att1.att_heads.1.linear2.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.norm1.weight 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.norm1.bias 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.norm2.weight 32 [32]\n",
      "encoder_layers.1.att1.att_heads.1.norm2.bias 32 [32]\n",
      "encoder_layers.1.ffn.0.weight 65536 [1024, 64]\n",
      "encoder_layers.1.ffn.0.bias 1024 [1024]\n",
      "encoder_layers.1.ffn.3.weight 65536 [64, 1024]\n",
      "encoder_layers.1.ffn.3.bias 64 [64]\n",
      "encoder_layers.1.norm1.weight 64 [64]\n",
      "encoder_layers.1.norm1.bias 64 [64]\n",
      "encoder_layers.1.norm2.weight 64 [64]\n",
      "encoder_layers.1.norm2.bias 64 [64]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.0.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.2.att1.att_heads.0.linear1.bias 512 [512]\n",
      "encoder_layers.2.att1.att_heads.0.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.2.att1.att_heads.0.linear2.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.norm1.weight 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.norm1.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.norm2.weight 32 [32]\n",
      "encoder_layers.2.att1.att_heads.0.norm2.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.k_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.k_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.v_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.v_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.q_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.q_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.out_proj.weight 1024 [32, 32]\n",
      "encoder_layers.2.att1.att_heads.1.self_attn.out_proj.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.linear1.weight 16384 [512, 32]\n",
      "encoder_layers.2.att1.att_heads.1.linear1.bias 512 [512]\n",
      "encoder_layers.2.att1.att_heads.1.linear2.weight 16384 [32, 512]\n",
      "encoder_layers.2.att1.att_heads.1.linear2.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.norm1.weight 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.norm1.bias 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.norm2.weight 32 [32]\n",
      "encoder_layers.2.att1.att_heads.1.norm2.bias 32 [32]\n",
      "encoder_layers.2.ffn.0.weight 65536 [1024, 64]\n",
      "encoder_layers.2.ffn.0.bias 1024 [1024]\n",
      "encoder_layers.2.ffn.3.weight 65536 [64, 1024]\n",
      "encoder_layers.2.ffn.3.bias 64 [64]\n",
      "encoder_layers.2.norm1.weight 64 [64]\n",
      "encoder_layers.2.norm1.bias 64 [64]\n",
      "encoder_layers.2.norm2.weight 64 [64]\n",
      "encoder_layers.2.norm2.bias 64 [64]\n",
      "TOTAL                1329984\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name:20} {param.numel()} {list(param.shape)}')\n",
    "print(f'TOTAL                {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1527917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T11:07:29.118169Z",
     "iopub.status.busy": "2022-04-18T11:07:29.117592Z",
     "iopub.status.idle": "2022-04-18T11:07:29.428339Z",
     "shell.execute_reply": "2022-04-18T11:07:29.428760Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.756514Z"
    },
    "papermill": {
     "duration": 0.360541,
     "end_time": "2022-04-18T11:07:29.428906",
     "exception": false,
     "start_time": "2022-04-18T11:07:29.068365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(train_accuracy)+1), train_accuracy)\n",
    "plt.plot(range(1, len(valid_accuracy)+1), valid_accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47364e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T20:22:30.25554Z",
     "iopub.status.busy": "2022-04-14T20:22:30.255054Z",
     "iopub.status.idle": "2022-04-14T20:22:30.458964Z",
     "shell.execute_reply": "2022-04-14T20:22:30.458276Z",
     "shell.execute_reply.started": "2022-04-14T20:22:30.255499Z"
    },
    "papermill": {
     "duration": 0.042764,
     "end_time": "2022-04-18T11:07:29.514337",
     "exception": false,
     "start_time": "2022-04-18T11:07:29.471573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([64, 128, 256], [99.7, 99.2, 98.9])\n",
    "plt.plot([64, 128, 256], [93.7, 93.8, 93.5])\n",
    "plt.legend([\"Training accuracy\", \"Testing accuracy\"])\n",
    "plt.title(\"Accuracies for different embedding sizes\")\n",
    "plt.xticks([64, 128, 256])\n",
    "plt.xlabel('Embedding size')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986425dd",
   "metadata": {
    "papermill": {
     "duration": 0.042075,
     "end_time": "2022-04-18T11:07:29.598717",
     "exception": false,
     "start_time": "2022-04-18T11:07:29.556642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Final model**  \n",
    "Finally, we create a model using all the training data and we generate the submission with the predicted test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42a2ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T11:07:29.688371Z",
     "iopub.status.busy": "2022-04-18T11:07:29.687463Z",
     "iopub.status.idle": "2022-04-18T12:52:51.004031Z",
     "shell.execute_reply": "2022-04-18T12:52:51.004462Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.758558Z"
    },
    "papermill": {
     "duration": 6321.363327,
     "end_time": "2022-04-18T12:52:51.004613",
     "exception": false,
     "start_time": "2022-04-18T11:07:29.641286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model for 25 epochs\n",
      "Train: wpb=9645, bsz=26, num_updates=4466\n",
      "| epoch 001 | train accuracy=38.635 (253s)\n",
      "| epoch 002 | train accuracy=77.788 (506s)\n",
      "| epoch 003 | train accuracy=82.453 (759s)\n",
      "| epoch 004 | train accuracy=84.313 (1012s)\n",
      "| epoch 005 | train accuracy=85.419 (1265s)\n",
      "| epoch 006 | train accuracy=86.268 (1518s)\n",
      "| epoch 007 | train accuracy=86.705 (1770s)\n",
      "| epoch 008 | train accuracy=87.205 (2023s)\n",
      "| epoch 009 | train accuracy=87.590 (2276s)\n",
      "| epoch 010 | train accuracy=87.715 (2529s)\n",
      "| epoch 011 | train accuracy=88.066 (2782s)\n",
      "| epoch 012 | train accuracy=88.139 (3034s)\n",
      "| epoch 013 | train accuracy=88.428 (3287s)\n",
      "| epoch 014 | train accuracy=88.557 (3541s)\n",
      "| epoch 015 | train accuracy=88.689 (3794s)\n",
      "| epoch 016 | train accuracy=88.809 (4046s)\n",
      "| epoch 017 | train accuracy=88.797 (4299s)\n",
      "| epoch 018 | train accuracy=89.066 (4552s)\n",
      "| epoch 019 | train accuracy=89.220 (4805s)\n",
      "| epoch 020 | train accuracy=89.220 (5057s)\n",
      "| epoch 021 | train accuracy=89.331 (5310s)\n",
      "| epoch 022 | train accuracy=89.340 (5563s)\n",
      "| epoch 023 | train accuracy=89.540 (5816s)\n",
      "| epoch 024 | train accuracy=89.588 (6068s)\n",
      "| epoch 025 | train accuracy=89.576 (6321s)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training final model for {epochs} epochs')\n",
    "model, optimizer = get_model(3)\n",
    "t0 = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    acc = train(model, optimizer, train_data + val_data, batch_size, token_size, log=epoch==1)\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={acc:.3f} ({time.time() - t0:.0f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c4a9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T12:52:51.114198Z",
     "iopub.status.busy": "2022-04-18T12:52:51.113482Z",
     "iopub.status.idle": "2022-04-18T12:52:51.115919Z",
     "shell.execute_reply": "2022-04-18T12:52:51.116421Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.760644Z"
    },
    "papermill": {
     "duration": 0.060619,
     "end_time": "2022-04-18T12:52:51.116567",
     "exception": false,
     "start_time": "2022-04-18T12:52:51.055948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    sindex = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            label = torch.max(answer, 1)[1].cpu().numpy()\n",
    "            # Save labels and sentences index\n",
    "            labels.append(label)\n",
    "            sindex += [d[1] for d in batch]\n",
    "    return np.array(sindex), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90f7a6",
   "metadata": {
    "papermill": {
     "duration": 0.050649,
     "end_time": "2022-04-18T12:52:51.218843",
     "exception": false,
     "start_time": "2022-04-18T12:52:51.168194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the test database we replace the label (language) with a sentence index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fab3b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T12:52:51.323911Z",
     "iopub.status.busy": "2022-04-18T12:52:51.323417Z",
     "iopub.status.idle": "2022-04-18T12:53:06.700533Z",
     "shell.execute_reply": "2022-04-18T12:53:06.700037Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.762693Z"
    },
    "papermill": {
     "duration": 15.431547,
     "end_time": "2022-04-18T12:53:06.700673",
     "exception": false,
     "start_time": "2022-04-18T12:52:51.269126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test_txt = open(f'{INPUTDIR}/x_test.txt').read().splitlines()\n",
    "x_test_idx = [np.array([char_vocab.token2idx[c] if c in char_vocab.token2idx else unk_index for c in line]) for line in x_test_txt]\n",
    "test_data = [(x, idx) for idx, x in enumerate(x_test_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce4eed",
   "metadata": {
    "papermill": {
     "duration": 0.051528,
     "end_time": "2022-04-18T12:53:06.805010",
     "exception": false,
     "start_time": "2022-04-18T12:53:06.753482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The sentence index is used to rearrange the labels in the original sentence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cddd498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T12:53:06.987431Z",
     "iopub.status.busy": "2022-04-18T12:53:06.986437Z",
     "iopub.status.idle": "2022-04-18T12:54:00.185334Z",
     "shell.execute_reply": "2022-04-18T12:54:00.184467Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.764722Z"
    },
    "papermill": {
     "duration": 53.322001,
     "end_time": "2022-04-18T12:54:00.185474",
     "exception": false,
     "start_time": "2022-04-18T12:53:06.863473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index, labels = test(model, test_data, batch_size, token_size)\n",
    "order = np.argsort(index)\n",
    "labels = labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93f93ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T12:54:00.291670Z",
     "iopub.status.busy": "2022-04-18T12:54:00.290909Z",
     "iopub.status.idle": "2022-04-18T12:54:00.428382Z",
     "shell.execute_reply": "2022-04-18T12:54:00.427953Z",
     "shell.execute_reply.started": "2022-04-17T21:09:24.767996Z"
    },
    "papermill": {
     "duration": 0.191956,
     "end_time": "2022-04-18T12:54:00.428498",
     "exception": false,
     "start_time": "2022-04-18T12:54:00.236542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,mwl\n",
      "1,nld\n",
      "2,ava\n",
      "3,kan\n",
      "4,bjn\n",
      "5,mon\n",
      "6,glk\n",
      "7,lez\n",
      "8,bul\n",
      "9,nan\n"
     ]
    }
   ],
   "source": [
    "with open('submission.csv', 'w') as f:\n",
    "    print('Id,Language', file=f)\n",
    "    for sentence_id, lang_id in enumerate(labels):\n",
    "        language = lang_vocab.idx2token[lang_id]\n",
    "        if sentence_id < 10:\n",
    "            print(f'{sentence_id},{language}')\n",
    "        print(f'{sentence_id},{language}', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11999.576166,
   "end_time": "2022-04-18T12:54:01.707922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-18T09:34:02.131756",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
